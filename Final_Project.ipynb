{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1v5GFV3J8P1GU0LNbZQ3kMT1oNN94TxDH",
      "authorship_tag": "ABX9TyNM9YqmpZv4/wbNHJDqE++g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaaath-i/emotion-classification-bert-models/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Project**"
      ],
      "metadata": {
        "id": "ox8Oq6QdEVib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task**: Sentiment Analysis - Emotion Classification\n",
        "\n",
        "*Dataset* : https://huggingface.co/datasets/mteb/emotion\n",
        "\n",
        "*Model 1* : https://huggingface.co/distilbert/distilbert-base-cased\n",
        "\n",
        "*Model 2* : https://huggingface.co/FacebookAI/roberta-base"
      ],
      "metadata": {
        "id": "TPL48DsAFI5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install bertviz transformers\n",
        "!pip install accelerate --upgrade"
      ],
      "metadata": {
        "id": "cqoXBsdSGWwl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ET8QymGBunhz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "hV6WVe7DHZ2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# distilbert-base-cased\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "print(tokenizer1)\n",
        "\n",
        "# roberta-base\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer2)"
      ],
      "metadata": {
        "id": "oQ4P2iNdIcUw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "kMVWocp6I7vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# distilbert-base-cased\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=6)\n",
        "\n",
        "# roberta-base\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base', num_labels=6)"
      ],
      "metadata": {
        "id": "nz5s-Rq5JAJq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "chfavJI7IXRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGgj6pXCEEKK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds = load_dataset(\"mteb/emotion\")\n",
        "\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()),\n",
        "        'label': example['label'],\n",
        "        'label_text': example['label_text']\n",
        "    }\n",
        "\n",
        "small_ds = DatasetDict(\n",
        "    train=ds['train'].shuffle(seed=24).select(range(800)).map(truncate),\n",
        "    val=ds['validation'].shuffle(seed=24).select(range(100)).map(truncate),\n",
        "    test=ds['test'].shuffle(seed=24).select(range(100)).map(truncate)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_ds"
      ],
      "metadata": {
        "id": "XA8WTWwJjy_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_ds[\"train\"][:10])"
      ],
      "metadata": {
        "id": "eJ9CpocTGznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training #1: distilbert-base-cased"
      ],
      "metadata": {
        "id": "lx2GsfIxkBc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above:\n",
        "\n",
        "```\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=6)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NgI1gurHlD_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "def tokenize_function1(examples):\n",
        "    return tokenizer1(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_ds = small_ds.map(tokenize_function1, batched=True, batch_size=10)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer1)"
      ],
      "metadata": {
        "id": "lETLLs4MiHU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=224):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "lAlI9GQSirUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "set_seed(224)\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=6)\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments1 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/distilbert_checkpoints\",\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=3,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer1 = Trainer(\n",
        "    model=model1,\n",
        "    args=arguments1,\n",
        "    train_dataset=small_tokenized_ds['train'],\n",
        "    eval_dataset=small_tokenized_ds['val'],\n",
        "    processing_class=tokenizer1,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "_eoJ81xplA23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1.train()"
      ],
      "metadata": {
        "id": "HNWcMXGRlmDY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "c413f70b-900f-4d8a-ab36-a70c97d252c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [240/240 00:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.998300</td>\n",
              "      <td>0.868674</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.406900</td>\n",
              "      <td>0.457512</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.355300</td>\n",
              "      <td>0.422741</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=240, training_loss=0.7374678904811541, metrics={'train_runtime': 34.732, 'train_samples_per_second': 69.101, 'train_steps_per_second': 6.91, 'total_flos': 38345837680800.0, 'train_loss': 0.7374678904811541, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "s7V_xgHcnioQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = trainer1.evaluate(small_tokenized_ds['test'])\n",
        "print(result1)"
      ],
      "metadata": {
        "id": "bPs5dhaGnkO1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "62fb1793-b473-4e76-ed2a-b05c9ba82797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.40671998262405396, 'eval_accuracy': 0.89, 'eval_runtime': 0.23, 'eval_samples_per_second': 434.721, 'eval_steps_per_second': 43.472, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "fine_tuned_model1 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/distilbert_checkpoints/checkpoint-160\")\n",
        "\n",
        "model_inputs = tokenizer1(\n",
        "    list(small_tokenized_ds['test']['text']),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "outputs = fine_tuned_model1(**model_inputs)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1).numpy()\n",
        "true_labels = small_tokenized_ds['test']['label']\n",
        "\n",
        "label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "print(\"DistilBERT - Per-Class Accuracy:\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for i, emotion in enumerate(label_names):\n",
        "    mask = np.array(true_labels) == i\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        class_acc = accuracy_score(\n",
        "            np.array(true_labels)[mask],\n",
        "            np.array(predictions)[mask]\n",
        "        )\n",
        "        print(f\"{emotion:10s}: {class_acc:.2%} ({mask.sum()} samples)\")\n",
        "    else:\n",
        "        print(f\"{emotion:10s}: No samples in test set\")"
      ],
      "metadata": {
        "id": "RoP67zrJTydi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be824df9-f7aa-4a72-a5af-c382c855fb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT - Per-Class Accuracy:\n",
            "----------------------------------------\n",
            "sadness   : 97.06% (34 samples)\n",
            "joy       : 91.67% (36 samples)\n",
            "love      : 90.91% (11 samples)\n",
            "anger     : 66.67% (12 samples)\n",
            "fear      : 100.00% (6 samples)\n",
            "surprise  : 0.00% (1 samples)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "fine_tuned_model1.to(device)\n",
        "fine_tuned_model1.eval()\n",
        "\n",
        "texts = list(small_ds['test']['text'])\n",
        "true_labels = list(small_ds['test']['label'])\n",
        "\n",
        "inputs = tokenizer1(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = fine_tuned_model1(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results_df = pd.DataFrame({\n",
        "    'Text': [text[:100] + \"...\" if len(text) > 100 else text for text in texts],\n",
        "    'True_Label': [label_names[label] for label in true_labels],\n",
        "    'Predicted_Label': [label_names[pred] for pred in predictions],\n",
        "    'Correct': ['✓' if true == pred else '✗' for true, pred in zip(true_labels, predictions)]\n",
        "})\n",
        "\n",
        "errors_df = results_df[results_df['Correct'] == '✗']\n",
        "print(\"\\nERRORS ONLY:\")\n",
        "print(\"-\"*110)\n",
        "print(errors_df)"
      ],
      "metadata": {
        "id": "kanitxtdU2q8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a4c58b-2108-4fba-b454-32dc3acef6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERRORS ONLY:\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "                                                                                                       Text  \\\n",
            "29                                                                         i feel so cold a href http irish   \n",
            "36                                           im able to refine my poses and concepts without feeling rushed   \n",
            "41  i used to be able to hang around talk with the cashier when i was putting away my money now i feel r...   \n",
            "53                                       i feel very strongly about supporting charities that help children   \n",
            "56                        i was somewhat coerced into this blog review so i feel a bit rushed and flustered   \n",
            "70                                 i feel no shame whatsoever in longing for iron man at my local cineworld   \n",
            "72  i feel like i know who most of them are by now and am starting to develop my likes and dislikes thou...   \n",
            "80  i really enjoy cabernet for how aggressive the flavors tend to be and while this isnt exactly a ligh...   \n",
            "90  i started feeling a little funny but this was not anxiety but at the time i didnt know so i started ...   \n",
            "99  i couldnt help feeling for him and this awful predicament he lives with on a daily and nightly basis...   \n",
            "\n",
            "   True_Label Predicted_Label Correct  \n",
            "29      anger         sadness       ✗  \n",
            "36      anger            fear       ✗  \n",
            "41      anger            fear       ✗  \n",
            "53        joy            love       ✗  \n",
            "56      anger            fear       ✗  \n",
            "70       love         sadness       ✗  \n",
            "72        joy         sadness       ✗  \n",
            "80        joy            love       ✗  \n",
            "90   surprise             joy       ✗  \n",
            "99    sadness           anger       ✗  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "21NylSBts7Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import re\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "import os"
      ],
      "metadata": {
        "id": "a6CKImbAs8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints = {\n",
        "    1: \"/content/drive/MyDrive/Colab Notebooks/distilbert_checkpoints/checkpoint-80\",\n",
        "    2: \"/content/drive/MyDrive/Colab Notebooks/distilbert_checkpoints/checkpoint-160\"\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/results_vis_distilbert\"\n",
        "\n",
        "for epoch, checkpoint_path in checkpoints.items():\n",
        "    fine_tuned_model1 = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "    fine_tuned_model1.to(device)\n",
        "    fine_tuned_model1.eval()\n",
        "\n",
        "    model_inputs = tokenizer1(list(small_tokenized_ds['test']['text']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = fine_tuned_model1(**model_inputs, output_hidden_states=True)\n",
        "\n",
        "    important_layers = [0, 3, 6]\n",
        "\n",
        "    for layer in important_layers:\n",
        "        path = f\"{base_path}/epoch_{epoch}\"\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        if not os.path.exists(path + '/layer_' + str(layer)):\n",
        "            os.mkdir(path + '/layer_' + str(layer))\n",
        "\n",
        "        example = 0\n",
        "        tensors = []\n",
        "        labels = []\n",
        "\n",
        "        while example in range(len(outputs['hidden_states'][layer])):\n",
        "            sp_token_position = 0\n",
        "            for token in model_inputs['input_ids'][example]:\n",
        "                if token == 101:\n",
        "                    tensor = outputs['hidden_states'][layer][example][sp_token_position]\n",
        "                    tensors.append(tensor)\n",
        "                    break\n",
        "                sp_token_position += 1\n",
        "\n",
        "            label = [small_tokenized_ds['test']['text'][example], str(small_tokenized_ds['test']['label'][example])]\n",
        "            labels.append(label)\n",
        "            example += 1\n",
        "\n",
        "        writer = SummaryWriter(path + '/layer_' + str(layer))\n",
        "        writer.add_embedding(torch.stack(tensors).cpu(), metadata=labels, metadata_header=['Text', 'Emotion'])\n",
        "        writer.close()\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtLXQP9UylKo",
        "outputId": "395a2cca-02f8-44d1-a6a2-58f930443341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "warning: Embedding dir exists, did you set global_step for add_embedding()?\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training #2: roberta-base"
      ],
      "metadata": {
        "id": "slC6SIKzo-I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above:\n",
        "\n",
        "\n",
        "```\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base', num_labels=6)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jYEHD_uDpYb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function2(examples):\n",
        "    return tokenizer2(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_ds = small_ds.map(tokenize_function2, batched=True, batch_size=10)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer2)"
      ],
      "metadata": {
        "id": "0-dGUarIpeIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "set_seed(224)\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base', num_labels=6)\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments2 = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/roberta_checkpoints\",\n",
        "    per_device_train_batch_size=10,\n",
        "    per_device_eval_batch_size=10,\n",
        "    logging_steps=5,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer2 = Trainer(\n",
        "    model=model2,\n",
        "    args=arguments2,\n",
        "    train_dataset=small_tokenized_ds['train'],\n",
        "    eval_dataset=small_tokenized_ds['val'],\n",
        "    processing_class=tokenizer2,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "X4GHbeI5pl2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd12d581-479f-4693-b839-a16f5bf41e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer2.train()"
      ],
      "metadata": {
        "id": "zugovRzwqYNP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "d9241466-fa08-4e93-e90a-2225fb488df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 02:13, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.066200</td>\n",
              "      <td>1.044380</td>\n",
              "      <td>0.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.819600</td>\n",
              "      <td>0.666348</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.621800</td>\n",
              "      <td>0.536554</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.150800</td>\n",
              "      <td>0.559649</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.316100</td>\n",
              "      <td>0.618552</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=400, training_loss=0.6573207901790739, metrics={'train_runtime': 133.4971, 'train_samples_per_second': 29.963, 'train_steps_per_second': 2.996, 'total_flos': 122458955266440.0, 'train_loss': 0.6573207901790739, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "aWxof9n_r9_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = trainer2.evaluate(small_tokenized_ds['test'])\n",
        "print(result2)"
      ],
      "metadata": {
        "id": "JFzlYlYzsWg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "6bc35c79-a790-4f8e-88d6-252ab68fc770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.43955665826797485, 'eval_accuracy': 0.86, 'eval_runtime': 0.5226, 'eval_samples_per_second': 191.335, 'eval_steps_per_second': 19.134, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "fine_tuned_model2 = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/roberta_checkpoints/checkpoint-320\")\n",
        "\n",
        "model_inputs = tokenizer2(\n",
        "    list(small_tokenized_ds['test']['text']),\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "outputs = fine_tuned_model2(**model_inputs)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1).numpy()\n",
        "true_labels = small_tokenized_ds['test']['label']\n",
        "\n",
        "label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "\n",
        "print(\"roBERTa - Per-Class Accuracy:\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "for i, emotion in enumerate(label_names):\n",
        "    mask = np.array(true_labels) == i\n",
        "\n",
        "    if mask.sum() > 0:\n",
        "        class_acc = accuracy_score(\n",
        "            np.array(true_labels)[mask],\n",
        "            np.array(predictions)[mask]\n",
        "        )\n",
        "        print(f\"{emotion:10s}: {class_acc:.2%} ({mask.sum()} samples)\")\n",
        "    else:\n",
        "        print(f\"{emotion:10s}: No samples in test set\")"
      ],
      "metadata": {
        "id": "cy3_YkUh6Iek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd43c95f-040b-41aa-db9d-ca73fd0d6830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "roBERTa - Per-Class Accuracy:\n",
            "----------------------------------------\n",
            "sadness   : 97.06% (34 samples)\n",
            "joy       : 94.44% (36 samples)\n",
            "love      : 54.55% (11 samples)\n",
            "anger     : 66.67% (12 samples)\n",
            "fear      : 83.33% (6 samples)\n",
            "surprise  : 0.00% (1 samples)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "fine_tuned_model2.to(device)\n",
        "fine_tuned_model2.eval()\n",
        "\n",
        "texts = list(small_ds['test']['text'])\n",
        "true_labels = list(small_ds['test']['label'])\n",
        "\n",
        "inputs = tokenizer2(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = fine_tuned_model2(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results_df = pd.DataFrame({\n",
        "    'Text': [text[:100] + \"...\" if len(text) > 100 else text for text in texts],\n",
        "    'True_Label': [label_names[label] for label in true_labels],\n",
        "    'Predicted_Label': [label_names[pred] for pred in predictions],\n",
        "    'Correct': ['✓' if true == pred else '✗' for true, pred in zip(true_labels, predictions)]\n",
        "})\n",
        "\n",
        "errors_df = results_df[results_df['Correct'] == '✗']\n",
        "print(\"\\nERRORS ONLY:\")\n",
        "print(\"-\"*110)\n",
        "print(errors_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10410ed3-0c9a-47df-8bc6-456a71577e6e",
        "id": "jOmNqT0c6_5n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERRORS ONLY:\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "                                                                                                       Text  \\\n",
            "11                                                                     i listen to it i feel all rebellious   \n",
            "12                               i feel a gentle tap and find flower child watching me her expression grave   \n",
            "25          im feeling angry at someone i do something thoughtful for her and my feelings toward her soften   \n",
            "29                                                                         i feel so cold a href http irish   \n",
            "49  i feel like the people that i myself love want and need don t talk to me and don t connect with me a...   \n",
            "68  i was so impressed with the show especially for hs and i was moved by these talented kids but then a...   \n",
            "70                                 i feel no shame whatsoever in longing for iron man at my local cineworld   \n",
            "71                                i notice how different this question is from why i am feeling so agitated   \n",
            "72  i feel like i know who most of them are by now and am starting to develop my likes and dislikes thou...   \n",
            "73  i were to go overseas or cross the border then i become a foreigner and will feel that way but never...   \n",
            "77                                                                      i feel if i am nagged i stop caring   \n",
            "90  i started feeling a little funny but this was not anxiety but at the time i didnt know so i started ...   \n",
            "98  i could empathize with tab because of raging hormones and the connection feeling like someone else g...   \n",
            "99  i couldnt help feeling for him and this awful predicament he lives with on a daily and nightly basis...   \n",
            "\n",
            "   True_Label Predicted_Label Correct  \n",
            "11      anger            fear       ✗  \n",
            "12       love             joy       ✗  \n",
            "25      anger             joy       ✗  \n",
            "29      anger         sadness       ✗  \n",
            "49      anger         sadness       ✗  \n",
            "68       love        surprise       ✗  \n",
            "70       love         sadness       ✗  \n",
            "71       fear           anger       ✗  \n",
            "72        joy         sadness       ✗  \n",
            "73       love         sadness       ✗  \n",
            "77       love             joy       ✗  \n",
            "90   surprise             joy       ✗  \n",
            "98        joy            love       ✗  \n",
            "99    sadness           anger       ✗  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "RTioyu3MvH9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints2 = {\n",
        "    1: \"/content/drive/MyDrive/Colab Notebooks/roberta_checkpoints/checkpoint-80\",\n",
        "    2: \"/content/drive/MyDrive/Colab Notebooks/roberta_checkpoints/checkpoint-160\",\n",
        "    4: \"/content/drive/MyDrive/Colab Notebooks/roberta_checkpoints/checkpoint-320\"\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/results_vis_roberta\"\n",
        "\n",
        "for epoch, checkpoint_path in checkpoints2.items():\n",
        "    fine_tuned_model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
        "    fine_tuned_model2.to(device)\n",
        "    fine_tuned_model2.eval()\n",
        "\n",
        "    model_inputs = tokenizer2(list(small_tokenized_ds['test']['text']), padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    outputs = fine_tuned_model2(**model_inputs, output_hidden_states=True)\n",
        "\n",
        "    important_layers = [0, 11]\n",
        "\n",
        "    for layer in important_layers:\n",
        "        path = f\"{base_path}/epoch_{epoch}\"\n",
        "\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "\n",
        "        if not os.path.exists(path + '/layer_' + str(layer)):\n",
        "            os.mkdir(path + '/layer_' + str(layer))\n",
        "\n",
        "        example = 0\n",
        "        tensors = []\n",
        "        labels = []\n",
        "\n",
        "        while example in range(len(outputs['hidden_states'][layer])):\n",
        "            sp_token_position = 0\n",
        "            for token in model_inputs['input_ids'][example]:\n",
        "                if token == 0:\n",
        "                    tensor = outputs['hidden_states'][layer][example][sp_token_position]\n",
        "                    tensors.append(tensor)\n",
        "                    break\n",
        "                sp_token_position += 1\n",
        "\n",
        "            label = [small_tokenized_ds['test']['text'][example], str(small_tokenized_ds['test']['label'][example])]\n",
        "            labels.append(label)\n",
        "            example += 1\n",
        "\n",
        "        writer = SummaryWriter(path + '/layer_' + str(layer))\n",
        "        writer.add_embedding(torch.stack(tensors).cpu(), metadata=labels, metadata_header=['Text', 'Emotion'])\n",
        "        writer.close()\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5mXKeOHDQ6B",
        "outputId": "6294434a-5572-432e-c27f-a5c2b933ddff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ]
    }
  ]
}