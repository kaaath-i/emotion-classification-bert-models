{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1v5GFV3J8P1GU0LNbZQ3kMT1oNN94TxDH",
      "authorship_tag": "ABX9TyObp4JkF8PN3b+yXLB/9IT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaaath-i/emotion-classification-bert-models/blob/main/Final_Project_ZEILNHOFER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VU Introduction to Computational Linguistics**\n",
        "\n",
        "Katharina Zeilnhofer"
      ],
      "metadata": {
        "id": "ePPzjx82EcQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Final Project**"
      ],
      "metadata": {
        "id": "ox8Oq6QdEVib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task**: Sentiment Analysis - Emotion Classification\n",
        "\n",
        "*Dataset* : https://huggingface.co/datasets/mteb/emotion\n",
        "\n",
        "*Model 1* : https://huggingface.co/distilbert/distilbert-base-cased\n",
        "\n",
        "*Model 2* : https://huggingface.co/FacebookAI/roberta-base"
      ],
      "metadata": {
        "id": "TPL48DsAFI5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install bertviz transformers\n",
        "!pip install accelerate --upgrade"
      ],
      "metadata": {
        "id": "cqoXBsdSGWwl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "hV6WVe7DHZ2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# distilbert-base-cased\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "print(tokenizer1)\n",
        "\n",
        "# roberta-base\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "print(tokenizer2)"
      ],
      "metadata": {
        "id": "oQ4P2iNdIcUw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "kMVWocp6I7vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# distilbert-base-cased\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=6)\n",
        "\n",
        "# roberta-base\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base', num_labels=6)"
      ],
      "metadata": {
        "id": "nz5s-Rq5JAJq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "chfavJI7IXRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGgj6pXCEEKK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds = load_dataset(\"mteb/emotion\")\n",
        "\n",
        "def truncate(example):\n",
        "    return {\n",
        "        'text': \" \".join(example['text'].split()),\n",
        "        'label': example['label'],\n",
        "        'label_text': example['label_text']\n",
        "    }\n",
        "\n",
        "small_ds = DatasetDict(\n",
        "    train=ds['train'].shuffle(seed=24).select(range(128)).map(truncate),\n",
        "    val=ds['validation'].shuffle(seed=24).select(range(32)).map(truncate),\n",
        "    test=ds['test'].shuffle(seed=24).select(range(32)).map(truncate)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_ds"
      ],
      "metadata": {
        "id": "XA8WTWwJjy_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_ds[\"train\"][:10])"
      ],
      "metadata": {
        "id": "eJ9CpocTGznU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training #1: distilbert-base-cased"
      ],
      "metadata": {
        "id": "lx2GsfIxkBc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above:\n",
        "\n",
        "```\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased\")\n",
        "\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-cased', num_labels=6)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NgI1gurHlD_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "def tokenize_function1(examples):\n",
        "    return tokenizer1(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_ds = small_ds.map(tokenize_function1, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer1)"
      ],
      "metadata": {
        "id": "lETLLs4MiHU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments1 = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer1 = Trainer(\n",
        "    model=model1,\n",
        "    args=arguments1,\n",
        "    train_dataset=small_tokenized_ds['train'],\n",
        "    eval_dataset=small_tokenized_ds['val'],\n",
        "    processing_class=tokenizer1,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "_eoJ81xplA23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer1.train()"
      ],
      "metadata": {
        "id": "HNWcMXGRlmDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "s7V_xgHcnioQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model1 = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-40\")\n",
        "\n",
        "result1 = trainer1.evaluate(small_tokenized_ds['test'])\n",
        "print(result1)"
      ],
      "metadata": {
        "id": "bPs5dhaGnkO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "model_inputs = tokenizer1(list(small_tokenized_ds['test']['text']), padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = fine_tuned_model1(**model_inputs, output_hidden_states=True)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "test_accuracy = metric.compute(predictions=predictions, references=small_tokenized_ds['test']['label'])\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "id": "jdlTWSITrfRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "21NylSBts7Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import re\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb"
      ],
      "metadata": {
        "id": "a6CKImbAs8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model1 = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer/checkpoint-40\")\n",
        "\n",
        "model_inputs = tokenizer1(list(small_tokenized_ds['val']['text']), padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = fine_tuned_model1(**model_inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "oEnUdF2rs9Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ET8QymGBunhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "path = \"results_vis\"\n",
        "layer=0\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "while layer in range(len(outputs['hidden_states'])):\n",
        "  if not os.path.exists(path+'/layer_' + str(layer)):\n",
        "    os.mkdir(path+'/layer_' + str(layer))\n",
        "\n",
        "  example = 0\n",
        "  tensors = []\n",
        "  labels = []\n",
        "\n",
        "  while example in range(len(outputs['hidden_states'][layer])):\n",
        "    sp_token_position = 0\n",
        "    for token in model_inputs['input_ids'][example]:\n",
        "      if token != 101:\n",
        "        sp_token_position += 1\n",
        "      else:\n",
        "        tensor = outputs['hidden_states'][layer][example][sp_token_position]\n",
        "        tensors.append(tensor)\n",
        "        break\n",
        "\n",
        "    label = [small_tokenized_ds['test']['text'][example],str(small_tokenized_ds['test']['label'][example])]\n",
        "    labels.append(label)\n",
        "    example +=1\n",
        "\n",
        "  writer=SummaryWriter(path+'/layer_' + str(layer))\n",
        "  writer.add_embedding(torch.stack(tensors), metadata=labels, metadata_header=['Text','Emotion'])\n",
        "\n",
        "  layer+=1"
      ],
      "metadata": {
        "id": "VKsjdy5WtXHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training #2: roberta-base"
      ],
      "metadata": {
        "id": "slC6SIKzo-I8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above:\n",
        "\n",
        "\n",
        "```\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
        "\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained('FacebookAI/roberta-base', num_labels=6)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jYEHD_uDpYb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function2(examples):\n",
        "    return tokenizer2(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "small_tokenized_ds = small_ds.map(tokenize_function2, batched=True, batch_size=16)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer2)"
      ],
      "metadata": {
        "id": "0-dGUarIpeIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "arguments = TrainingArguments(\n",
        "    output_dir=\"sample_cl_trainer2\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_steps=8,\n",
        "    num_train_epochs=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none',\n",
        "    seed=224\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer2 = Trainer(\n",
        "    model=model2,\n",
        "    args=arguments,\n",
        "    train_dataset=small_tokenized_ds['train'],\n",
        "    eval_dataset=small_tokenized_ds['val'],\n",
        "    processing_class=tokenizer1,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "X4GHbeI5pl2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer2.train()"
      ],
      "metadata": {
        "id": "zugovRzwqYNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "aWxof9n_r9_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model2 = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer2/checkpoint-40\")\n",
        "\n",
        "result2 = trainer2.evaluate(small_tokenized_ds['test'])\n",
        "print(result2)"
      ],
      "metadata": {
        "id": "JFzlYlYzsWg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "fine_tuned_model2 = AutoModelForSequenceClassification.from_pretrained(\"sample_cl_trainer2/checkpoint-40\")\n",
        "\n",
        "model_inputs = tokenizer2(list(small_tokenized_ds['test']['text']), padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = fine_tuned_model2(**model_inputs, output_hidden_states=True)\n",
        "predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "accuracy = metric.compute(predictions=predictions, references=small_tokenized_ds['test']['label'])\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "dUoxXlQNsADP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "RTioyu3MvH9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer2(list(small_tokenized_ds['val']['text']), padding=True, truncation=True, return_tensors='pt')\n",
        "outputs = fine_tuned_model2(**model_inputs, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "H0-5Gdv5vJS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"results_vis\"\n",
        "layer=0\n",
        "if not os.path.exists(path):\n",
        "  os.mkdir(path)\n",
        "\n",
        "while layer in range(len(outputs['hidden_states'])):\n",
        "  if not os.path.exists(path+'/layer_' + str(layer)):\n",
        "    os.mkdir(path+'/layer_' + str(layer))\n",
        "\n",
        "  example = 0\n",
        "  tensors = []\n",
        "  labels = []\n",
        "\n",
        "  while example in range(len(outputs['hidden_states'][layer])):\n",
        "    tensor = outputs['hidden_states'][layer][example][0]\n",
        "    tensors.append(tensor)\n",
        "\n",
        "    label = [small_tokenized_ds['test']['text'][example],str(small_tokenized_ds['test']['label'][example])]\n",
        "    labels.append(label)\n",
        "    example +=1\n",
        "\n",
        "  writer=SummaryWriter(path+'/layer_' + str(layer))\n",
        "  writer.add_embedding(torch.stack(tensors), metadata=labels, metadata_header=['Text','Emotion'])\n",
        "\n",
        "  layer+=1"
      ],
      "metadata": {
        "id": "_VEbs5LsvMDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}